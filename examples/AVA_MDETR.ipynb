{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Hdu9kzbK8Y"
      },
      "source": [
        "# **MDETR**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDwl2ZaxUSAN"
      },
      "source": [
        "## *Preliminaries*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtpOJfccUGl-"
      },
      "outputs": [],
      "source": [
        "! pip install timm transformers googledrivedownloader opencv-python matplotlib scikit-image\n",
        "\n",
        "import cv2\n",
        "import csv\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "import torch\n",
        "import requests\n",
        "import json\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "from skimage.measure import find_contours\n",
        "from matplotlib import patches,  lines\n",
        "from matplotlib.patches import Polygon\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "\n",
        "torch.set_grad_enabled(False);\n",
        "# MDETR\n",
        "transform = T.Compose([\n",
        "    T.Resize(800),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "# standard PyTorch mean-std input image normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utouTsXXEpnj"
      },
      "outputs": [],
      "source": [
        "gdd.download_file_from_google_drive(file_id='11BlSLeDE1dpa5Sm71ef4jYJ0OtPSqSEk',\n",
        "                                    dest_path='content/AVA_Metrics/AVA_Metrics.zip',\n",
        "                                    unzip=True)\n",
        "# download binaries to be used in the metrics\n",
        "\n",
        "with open('content/AVA_Metrics/ava_v2.2/ava_val_v2.2.csv', mode='r+') as csv_file:\n",
        "  with open('content/video_ID.csv', mode='w') as csv_test:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    line_count = 0\n",
        "    for_comparison = 0\n",
        "    for row in csv_reader:\n",
        "        if row[0] != for_comparison:\n",
        "          csv_test.write(f\"{row[0]}\\n\")\n",
        "          line_count += 1\n",
        "          for_comparison = row[0]\n",
        "    print(f'Processed {line_count} videos.')\n",
        "# Because of not having the URL to download video_IDs directly from AVA webpage, it creates this file from the AVA_validation file .\n",
        "\n",
        "open(\"MDETR_results.csv\", \"w\")\n",
        "# Create file for results.\n",
        "\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1m6jU4EI-3wgzSg_A-0OnDUh-clgnYUxo' -O content/ava_train_v2.2.csv\n",
        "# Download CSV to compare\n",
        "\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1nNUm7EDnd24Y95CE2oxC-LwcQimljKgR' -O content/questions_list.txt\n",
        "# txt file that saves questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf8yN7cyVcaQ"
      },
      "source": [
        "## *Define model*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QODSn-WynTN"
      },
      "outputs": [],
      "source": [
        "model_qa = torch.hub.load('ashkamath/mdetr:main', 'mdetr_efficientnetB5_gqa', pretrained=True, return_postprocessor=False)\n",
        "model_qa = model_qa.cuda()\n",
        "model_qa.eval()\n",
        "\n",
        "# We download the mapping from the answers to their id.\n",
        "answer2id_by_type = json.load(requests.get(\"https://nyu.box.com/shared/static/j4rnpo8ixn6v0iznno2pim6ffj3jyaj8.json\", stream=True).raw)\n",
        "id2answerbytype = {}                                                       \n",
        "for ans_type in answer2id_by_type.keys():                        \n",
        "    curr_reversed_dict = {v: k for k, v in answer2id_by_type[ans_type].items()}\n",
        "    id2answerbytype[ans_type] = curr_reversed_dict              "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4A5q5t7ZhEC"
      },
      "source": [
        "## *Process*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCehpVvWfIaz"
      },
      "source": [
        "### Boxes and plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhXozaAnEB-F"
      },
      "outputs": [],
      "source": [
        "# colors for visualization\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "\n",
        "def apply_mask(image, mask, color, alpha=0.5):\n",
        "    \"\"\"Apply the given mask to the image.\n",
        "    \"\"\"\n",
        "    for c in range(3):\n",
        "        image[:, :, c] = np.where(mask == 1,\n",
        "                                  image[:, :, c] *\n",
        "                                  (1 - alpha) + alpha * color[c] * 255,\n",
        "                                  image[:, :, c])\n",
        "    return image\n",
        "\n",
        "# for output bounding box post-processing\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = box_cxcywh_to_xyxy(out_bbox)\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b\n",
        "\n",
        "def plot_results(pil_img, scores, boxes, masks=None):\n",
        "    plt.figure(figsize=(16,10))\n",
        "    np_image = np.array(pil_img)\n",
        "    ax = plt.gca()\n",
        "    colors = COLORS * 100\n",
        "    if masks is None:\n",
        "      masks = [None for _ in range(len(scores))]\n",
        "    assert len(scores) == len(boxes) == len(masks)\n",
        "    for s, (xmin, ymin, xmax, ymax), mask, c in zip(scores, boxes.tolist(), masks, colors):\n",
        "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                   fill=False, color=c, linewidth=3))\n",
        "\n",
        "        if mask is None:\n",
        "          continue\n",
        "        np_image = apply_mask(np_image, mask, c)\n",
        "\n",
        "        padded_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
        "        padded_mask[1:-1, 1:-1] = mask\n",
        "        contours = find_contours(padded_mask, 0.5)\n",
        "        for verts in contours:\n",
        "          # Subtract the padding and flip (y, x) to (x, y)\n",
        "          verts = np.fliplr(verts) - 1\n",
        "          p = Polygon(verts, facecolor=\"none\", edgecolor=c)\n",
        "          ax.add_patch(p)\n",
        "\n",
        "\n",
        "    plt.imshow(np_image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def plot_image(pil_img):\n",
        "    plt.figure(figsize=(16,10))\n",
        "    np_image = np.array(pil_img)\n",
        "    plt.imshow(np_image)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NRv6IDvfpag"
      },
      "source": [
        "### Video Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdzR45MUy5jC"
      },
      "outputs": [],
      "source": [
        "def get_data_from_video(im, caption):\n",
        "  # mean-std normalize the input image (batch-size: 1)\n",
        "  img = transform(im).unsqueeze(0).cuda()\n",
        "\n",
        "  # propagate through the model\n",
        "  memory_cache = model_qa(img, [caption], encode_and_save=True)\n",
        "  outputs = model_qa(img, [caption], encode_and_save=False, memory_cache=memory_cache)\n",
        "\n",
        "  # keep only predictions with 0.7+ confidence\n",
        "  probas = 1 - outputs['pred_logits'].softmax(-1)[0, :, -1].cpu()\n",
        "  keep = (probas > CONFIDENCE_TOL).cpu()\n",
        "\n",
        "  # Classify the question type\n",
        "  type_conf, type_pred = outputs[\"pred_answer_type\"].softmax(-1).max(-1)\n",
        "  ans_type = type_pred.item()\n",
        "  types = [\"obj\", \"attr\", \"rel\", \"global\", \"cat\"]\n",
        "\n",
        "  ans_conf, ans = outputs[f\"pred_answer_{types[ans_type]}\"][0].softmax(-1).max(-1)\n",
        "  answer = id2answerbytype[f\"answer_{types[ans_type]}\"][ans.item()]\n",
        "\n",
        "  if answer == \"yes\":\n",
        "    # convert boxes from [0; 1] to image scales\n",
        "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'].cpu()[0, keep], im.size)\n",
        "    # plot_results(im, probas[keep], bboxes_scaled)\n",
        "\n",
        "    return box_cxcywh_to_xyxy(outputs['pred_boxes'].cpu()[0, keep]).tolist()\n",
        "    # MDETR returns x and y initial, and width and height, so we convert\n",
        "    # it to initial x and y, and end x and y\n",
        "\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQNZIkGuX2sS"
      },
      "outputs": [],
      "source": [
        "def define_question(actions_filename, action_id=None):\n",
        "  \"\"\"\n",
        "  Defines which question use to search for the correct\n",
        "  action, given the action ID, using a questions list file.\n",
        "  \"\"\"\n",
        "  with open(actions_filename) as file_handler:\n",
        "    file_content = file_handler.read()\n",
        "\n",
        "  file_content = file_content.split(\"\\n\")\n",
        "\n",
        "  if action_id != None:\n",
        "    question = file_content[action_id - 1]\n",
        "    # Actions ID start with 1, list with 0\n",
        "\n",
        "    return question\n",
        "\n",
        "  # If action_id is defined, return the questions asked\n",
        "  # Else, returns questions list\n",
        "\n",
        "  return file_content\n",
        "\n",
        "def append_boxes_frames(im, captions):\n",
        "  frames = list()\n",
        "  action_ids = list()\n",
        "\n",
        "  for caption in captions:\n",
        "    boxes_axes = get_data_from_video(im, caption)\n",
        "\n",
        "    if boxes_axes != None:\n",
        "      for box in boxes_axes:\n",
        "        if len(box) > 1:\n",
        "          frames.append(box)\n",
        "          action_ids.append(captions.index(caption))\n",
        "\n",
        "  return frames, action_ids\n",
        "\n",
        "def get_n_frames(video):\n",
        "  return int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  # Metadata get from OpenCV\n",
        "\n",
        "def process_video(path, captions):\n",
        "  global succeed_frames\n",
        "  \n",
        "  capture = cv2.VideoCapture(path)\n",
        "  succeed_frames = list()\n",
        "\n",
        "  if isinstance(get_n_frames(capture), int):\n",
        "  # Check if the video's url has frames verifying that they are valid\n",
        "\n",
        "    FPS = round(capture.get(cv2.CAP_PROP_FPS)) or 30\n",
        "    # In this way, we use video FPS, or 30 if get video FPS fails\n",
        "    for frame_index in range(902, 1798, 1):\n",
        "      # Step must be number of fps\n",
        "      capture.set(1, frame_index * FPS + int(FPS / 2))\n",
        "      # AVA uses the middle frame\n",
        "      success, frame = capture.read()\n",
        "\n",
        "      if success:\n",
        "        im = Image.fromarray(frame)\n",
        "\n",
        "        boxes, actions = append_boxes_frames(im, captions)\n",
        "\n",
        "        for box, action in zip(boxes, actions):\n",
        "          succeed_frames.append(\n",
        "              [\n",
        "                  int(frame_index), # Data saved in seconds from start\n",
        "                  *box, # Unpack frame coords\n",
        "                  action, # ACTION ID for each frame\n",
        "                  boxes.index(box) # In the same frame, box number get in order\n",
        "              ]\n",
        "          )\n",
        "        \n",
        "        print('frame',frame_index, 'of 1798.', end='\\r')\n",
        "\n",
        "  capture.release()\n",
        "\n",
        "  return succeed_frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDCzMuxUdsZZ"
      },
      "outputs": [],
      "source": [
        "def write_csv(name, video_data):\n",
        "  \"\"\"\n",
        "  Receives video content and writes it in a CSV file.\n",
        "  \"\"\"\n",
        "  with open(\"MDETR_results.csv\", \"a\") as file_handler:\n",
        "      for video_data_list in video_data:\n",
        "          new_list = [name, *video_data_list]\n",
        "          text_line = \"{},{:04d},{:.3f},{:.3f},{:.3f},{:.3f},{},{}\\n\".format(*new_list)\n",
        "          file_handler.write(text_line)\n",
        "          # Parameters written\n",
        "          # 1. Video name\n",
        "          # 2. Time of event\n",
        "          # 3. x of first extreme of rectangle\n",
        "          # 4. y of first extreme of rectangle\n",
        "          # 5. x of second extreme of rectangle\n",
        "          # 6. y of second extreme of rectangle\n",
        "          # 7. Action ID\n",
        "          # 8. Index to recognize rects if there are more than one in an image\n",
        "\n",
        "def iter_videos(flag, N_VIDEOS_TEST):\n",
        "  \"\"\"\n",
        "  Gets a list of video names in an url and analyze each one. Deletes\n",
        "  not in use files to save space.\n",
        "  \"\"\"\n",
        "  format_videos = [\".mp4\", \".mkv\", \".webm\"]\n",
        "\n",
        "  questions = define_question(\"content/questions_list.txt\")\n",
        "\n",
        "  with open('content/video_ID.csv', mode='r') as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "      line_count = 0\n",
        "\n",
        "      for row in csv_reader:\n",
        "         for format_video in format_videos:\n",
        "            name = row[0]\n",
        "            url = \"https://s3.amazonaws.com/ava-dataset/trainval/\"\n",
        "            try:\n",
        "                video_data = process_video(url + name + format_video, questions)\n",
        "                # Returns list of frames that answer question\n",
        "                write_csv(name, video_data)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "\n",
        "         line_count += 1 \n",
        "         if (flag==False) and (line_count >= N_VIDEOS_TEST):\n",
        "                break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GymnvjE3f47Q"
      },
      "source": [
        "## *Main Sets*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3yVCibHbTtj"
      },
      "outputs": [],
      "source": [
        "CONFIDENCE_TOL = 0.7\n",
        "N_VIDEOS_TEST = 1\n",
        "# With this var we select number of videos tested.\n",
        "\n",
        "os.remove('MDETR_results.csv')\n",
        "# Delete previous file to not append results. Use it only if the test runs complete.\n",
        "iter_videos(False, N_VIDEOS_TEST)\n",
        "# 'True' if you want to analize all videos.\n",
        "# 'False' if you want to analize N_VIDEOS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUppzV_cbPbi"
      },
      "source": [
        "# **Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwnBTteRbvjH"
      },
      "outputs": [],
      "source": [
        "!python -O content/AVA_Metrics/calc_mAP.py -l content/AVA_Metrics/ava_v2.2/ava_action_list_v2.2_for_activitynet_2019.pbtxt  -g content/AVA_Metrics/ava_v2.2/ava_val_v2.2.csv -e content/AVA_Metrics/ava_v2.2/ava_val_excluded_timestamps_v2.2.csv -d MDETR_results.csv"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "DDwl2ZaxUSAN",
        "lf8yN7cyVcaQ",
        "s4A5q5t7ZhEC",
        "MCehpVvWfIaz",
        "3NRv6IDvfpag",
        "GymnvjE3f47Q",
        "xUppzV_cbPbi"
      ],
      "name": "AVA_MDETR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}